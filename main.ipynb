{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from conlleval import evaluate\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "ninf = -1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def read_data(path, column=0):\n",
    "    \"\"\"column=0 means input sequence, column=1 means label\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    data = []\n",
    "    sample = []\n",
    "    \n",
    "    for line in lines:\n",
    "        formatted_line = line.strip()\n",
    "        \n",
    "        if len(formatted_line) > 0:\n",
    "            split_data = formatted_line.split(\" \")\n",
    "            sample.append(split_data[column])\n",
    "\n",
    "        else:\n",
    "            data.append(sample)\n",
    "            sample = []\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "full_dir = Path('full')\n",
    "partial_dir = Path('partial')\n",
    "\n",
    "x_data, y_data = read_data(partial_dir/'train', 0), read_data(partial_dir/'train', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 700)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of instances in the dataset\n",
    "len(x_data), len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y labels\n",
    "y_vocab = sorted(list(set([oo for o in y_data for oo in o]))); y_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4068"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x vocab\n",
    "x_vocab = list(set([oo for o in x_data for oo in o])); len(x_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part I (i): Emission scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def calc_e(x_data, y_data, x_vocab, y_vocab):\n",
    "    count_emission = Counter([(x,y) for x_instance, y_instance in zip(x_data, y_data) for x, y in zip(x_instance, y_instance)])\n",
    "    count_label = Counter([oo for o in y_data for oo in o])\n",
    "    \n",
    "    e_score = {}\n",
    "    for y in y_vocab:\n",
    "        for x in x_vocab:\n",
    "            feature = f\"emission:{y}+{x}\"\n",
    "            \n",
    "            if (x,y) not in count_emission:\n",
    "                e_score[feature] = ninf\n",
    "            else:\n",
    "                score = np.log(count_emission[(x,y)]  /  count_label[y])\n",
    "                e_score[feature] = score\n",
    "    \n",
    "    return e_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "emission_dict = calc_e(x_data, y_data, x_vocab, y_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part I (ii): Transition scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def calc_t(y_data, y_vocab):\n",
    "    count_transition = Counter([ (y_prev, y) for y_instance in y_data for y_prev, y in zip(['START'] + y_instance, y_instance + ['STOP'])])\n",
    "    count_label = Counter([y for y_instance in y_data for y in ['START'] + y_instance])\n",
    "    \n",
    "    f_score = {}\n",
    "    for y_prev in ['START'] + y_vocab:\n",
    "        for y in y_vocab + ['STOP']:\n",
    "            feature = f\"transition:{y_prev}+{y}\"\n",
    "            \n",
    "            if (y_prev,y) not in count_transition:\n",
    "                f_score[feature] = ninf\n",
    "            else:\n",
    "                score = np.log(count_transition[(y_prev,y)]  /  count_label[y_prev])\n",
    "                f_score[feature] = score\n",
    "    \n",
    "    return f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "transition_dict = calc_t(y_data, y_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "feature_dict = {**transition_dict, **emission_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part II (i): Compute Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def compute_score(x_instance, y_instance, feature_dict):\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    for x, y in zip(x_instance, y_instance): feature_count[f\"emission:{y}+{x}\"] += 1\n",
    "    \n",
    "    for y_prev, y in zip(['START'] + y_instance, y_instance + ['STOP']):\n",
    "        feature_count[f\"transition:{y_prev}+{y}\"] += 1\n",
    "        \n",
    "    score = sum([feature_dict[feat]*count for feat, count in feature_count.items()])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-139.57175855522826"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_instance = \"This is the second U.N.-Congolese offensive against militias in the region since the DRC 's constitutional referendum a week ago .\".split()\n",
    "y_instance = 'O O O O O O O O O O O O O B-geo O O O O O O O'.split()\n",
    "compute_score(x_instance, y_instance, feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part II (ii): Viterbi decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O B-geo O O O O O O O\n",
      "O O O O O O O O O O O O O B-geo O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "def viterbi(x_instance, y_vocab, feature_dict):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.full( (n,d), ninf)\n",
    "    bp = np.full( (n,d), 0, dtype=np.int)\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:START+{y}\",  ninf)\n",
    "        e_score = feature_dict.get( f\"emission:{y}+{x_instance[0]}\",  ninf)\n",
    "        scores[0, i] = t_score + e_score\n",
    "        \n",
    "    for i in range(1, n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            for y_prev_i, y_prev in enumerate(y_vocab):\n",
    "                t_score = feature_dict.get( f\"transition:{y_prev}+{y}\", ninf)\n",
    "                e_score = feature_dict.get( f\"emission:{y}+{x_instance[i]}\", ninf)\n",
    "                score = t_score + e_score + scores[i-1, y_prev_i]\n",
    "                if score > scores[i, y_i]:\n",
    "                    scores[i, y_i] = score\n",
    "                    bp[i, y_i] = y_prev_i\n",
    "    \n",
    "    final_score, final_bp = ninf, 0\n",
    "    for i, y_prev in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:{y_prev}+STOP\", ninf)\n",
    "        score = t_score + scores[n-1, i]\n",
    "        if score > final_score: \n",
    "            final_score = score\n",
    "            final_bp = i\n",
    "            \n",
    "    decoded_sequence = [ y_vocab[final_bp], ]\n",
    "    for i in range(n-1, 0, -1):\n",
    "        final_bp = bp[i, final_bp]\n",
    "        decoded_sequence = [ y_vocab[final_bp] ] + decoded_sequence\n",
    "        \n",
    "    return decoded_sequence\n",
    "\n",
    "print(\" \".join(viterbi(x_instance, y_vocab, feature_dict)),)\n",
    "print(\" \".join(y_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(in_file_path, y_vocab, feature_dict, out_file_path):\n",
    "    x_data = read_data(in_file_path, column=0)\n",
    "    with open(out_file_path, 'w') as f:\n",
    "        for x_instance in x_data:\n",
    "            pred = viterbi(x_instance, y_vocab, feature_dict)\n",
    "            for word, label in zip(x_instance, pred): f.write(f\"{word} {label} \\n\")\n",
    "            f.write('\\n')\n",
    "\n",
    "inference(partial_dir/'dev.in', y_vocab, feature_dict, partial_dir/'dev.p2.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part III (i): CRF loss\n",
    "refer to [machine learning slides](https://drive.google.com/file/d/1RfPcnQigx4jdLtnTjjjI1Jgd2UufexhV/view?usp=sharing) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def logsumexp(a):\n",
    "    b = a.max()\n",
    "    return  b + np.log( (np.exp(a-b)).sum() )\n",
    "\n",
    "def forward(x_instance, y_vocab, feature_dict):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.zeros( (n,d) )\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:START+{y}\", ninf)\n",
    "        scores[0, i] = t_score\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            temp = []\n",
    "            for y_prev_i, y_prev in enumerate(y_vocab):\n",
    "                t_score = feature_dict.get( f\"transition:{y_prev}+{y}\", ninf)\n",
    "                e_score = feature_dict.get( f\"emission:{y_prev}+{x_instance[i-1]}\", ninf)\n",
    "                temp.append(e_score + t_score + scores[i-1, y_prev_i])\n",
    "            scores[i, y_i] = logsumexp(np.array(temp))\n",
    "    \n",
    "    temp = []\n",
    "    for i, y_prev in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:{y_prev}+STOP\", ninf)\n",
    "        e_score = feature_dict.get( f\"emission:{y_prev}+{x_instance[-1]}\", ninf)\n",
    "        temp.append(e_score + t_score + scores[-1, i])\n",
    "    alpha = logsumexp(np.array(temp))\n",
    "    \n",
    "    return scores, alpha\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn_instance(x_instance, y_instance, feature_dict, y_vocab):\n",
    "    first_term = compute_score(x_instance, y_instance, feature_dict)\n",
    "    _, forward_score = forward(x_instance, y_vocab, feature_dict)\n",
    "    return forward_score - first_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3302877199753311"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn_instance(x_instance, y_instance, feature_dict, y_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part III (ii): forward backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def backward(x_instance, y_vocab, feature_dict, aggreg_fn=logsumexp):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.zeros( (n,d) )\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:{y}+STOP\", ninf)\n",
    "        e_score = feature_dict.get( f\"emission:{y}+{x_instance[-1]}\", ninf)\n",
    "        scores[-1, i] = t_score + e_score\n",
    "        \n",
    "    for i in range(n-1, 0, -1):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            temp = []\n",
    "            for y_next_i, y_next in enumerate(y_vocab):\n",
    "                t_score = feature_dict.get( f\"transition:{y}+{y_next}\", ninf)\n",
    "                e_score = feature_dict.get( f\"emission:{y}+{x_instance[i-1]}\")\n",
    "                temp.append(e_score + t_score + scores[i, y_next_i])\n",
    "            scores[i-1, y_i] = aggreg_fn(np.array(temp))\n",
    "            \n",
    "    temp = []\n",
    "    for i, y_next in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:START+{y_next}\")\n",
    "        temp.append(t_score + scores[0, i])\n",
    "    beta = aggreg_fn(np.array(temp))\n",
    "    \n",
    "    return scores, beta\n",
    "\n",
    "\n",
    "\n",
    "def forward_backward(x_instance, y_vocab, feature_dict):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    f_scores, alpha = forward(x_instance, y_vocab, feature_dict)\n",
    "    b_scores, beta = backward(x_instance, y_vocab, feature_dict)\n",
    "    \n",
    "    feature_expected_count = defaultdict(float)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            e_feature = f\"emission:{y}+{x_instance[i]}\"\n",
    "            feature_expected_count[e_feature] += np.exp(f_scores[i, y_i] + b_scores[i, y_i] - alpha)\n",
    "            \n",
    "    for i, y_next in enumerate(y_vocab):\n",
    "        t_feature = f\"transition:START+{y_next}\"\n",
    "        feature_expected_count[t_feature] += np.exp(f_scores[0, i] + b_scores[0, i] - alpha)\n",
    "        \n",
    "        t_feature = f\"transition:{y_next}+STOP\"\n",
    "        feature_expected_count[t_feature] += np.exp(f_scores[-1, i] + b_scores[-1, i] - alpha)\n",
    "        \n",
    "    for y_i, y in enumerate(y_vocab):\n",
    "        for y_next_i, y_next in enumerate(y_vocab):\n",
    "            t_feature = f\"transition:{y}+{y_next}\"\n",
    "            t_score = feature_dict.get(t_feature, ninf)\n",
    "            total = 0\n",
    "            for i in range(n-1):\n",
    "                e_score = feature_dict.get(f\"emission:{y}+{x_instance[i]}\", ninf)\n",
    "                total += np.exp(f_scores[i, y_i] + b_scores[i+1, y_next_i] + t_score + e_score - alpha)\n",
    "            feature_expected_count[t_feature] = total\n",
    "            \n",
    "    return feature_expected_count\n",
    "\n",
    "def get_feature_count(x_instance, y_instance, feature_dict):\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    for x, y in zip(x_instance, y_instance): feature_count[f\"emission:{y}+{x}\"] += 1\n",
    "    \n",
    "    for y_prev, y in zip(['START'] + y_instance, y_instance + ['STOP']):\n",
    "        feature_count[f\"transition:{y_prev}+{y}\"] += 1\n",
    "    \n",
    "    return feature_count\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part IV (i): gradient and training with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def gradient_fn(x_data, y_data, feature_dict, y_vocab, eta=0):\n",
    "    feature_grad = defaultdict(float)\n",
    "    \n",
    "    for x_instance, y_instance in zip(x_data, y_data):\n",
    "        feature_expected_counts = forward_backward(x_instance, y_vocab, feature_dict)\n",
    "        feature_actual_counts = get_feature_count(x_instance, y_instance, feature_dict)\n",
    "        for k, v in feature_expected_counts.items(): feature_grad[k] += v\n",
    "        for k, v in feature_actual_counts.items(): feature_grad[k] -= v    \n",
    "    \n",
    "    if eta > 0: \n",
    "        for k, v in feature_dict.items(): feature_grad[k] += 2*eta*v\n",
    "    \n",
    "    return feature_grad\n",
    "        \n",
    "    \n",
    "def loss_fn(x_data, y_data, feature_dict, y_vocab, eta=0):\n",
    "    loss = 0\n",
    "    for x_instance, y_instance in zip(x_data, y_data):\n",
    "        loss += loss_fn_instance(x_instance, y_instance, feature_dict, y_vocab) \n",
    "    reg_loss = eta * sum([o**2 for o in feature_dict.values()]) if eta > 0 else 0\n",
    "    return loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient verification\n",
    "feature_key_checks = ['emission:O+the', 'transition:START+O', 'transition:O+O', 'transition:I-per+I-per']\n",
    "feature_gradients = gradient_fn(x_data, y_data, feature_dict, y_vocab, eta=0)\n",
    "loss1 = loss_fn(x_data, y_data, feature_dict, y_vocab, eta=0)\n",
    "delta = 1e-6\n",
    "for feat_k in feature_key_checks:\n",
    "    new_feature_dict = feature_dict.copy()\n",
    "    new_feature_dict[feat_k] += delta\n",
    "    loss2 = loss_fn(x_data, y_data, new_feature_dict, y_vocab, eta=0)\n",
    "    numerical_grad = (loss2 - loss1) / delta\n",
    "    analytic_grad = feature_gradients[feat_k]\n",
    "    assert abs(numerical_grad - analytic_grad) / max(abs(numerical_grad), 1e-8) < 1e-5, 'Did not pass gradient checking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def numpy_to_dict(weight, feature_dict):\n",
    "    for i,k in enumerate(feature_dict.keys()):\n",
    "        feature_dict[k] = weight[i]\n",
    "    return feature_dict\n",
    "\n",
    "def dict_to_numpy(grads, feature_dict):\n",
    "    np_grads = np.zeros(len(feature_dict))\n",
    "    for i, k in enumerate(feature_dict.keys()):\n",
    "        np_grads[i] = grads[k]\n",
    "    return np_grads\n",
    "\n",
    "def get_loss_grad(weight, *args):\n",
    "    x_data, y_data, feature_dict, y_vocab = args\n",
    "    feature_dict = numpy_to_dict(weight, feature_dict)\n",
    "    loss = loss_fn(x_data, y_data, feature_dict, y_vocab, eta=0.1)\n",
    "    grads = gradient_fn(x_data, y_data, feature_dict, y_vocab, eta=0.1)\n",
    "    grads = dict_to_numpy(grads, feature_dict)\n",
    "    return loss, grads\n",
    "\n",
    "def callbackF(weight): print(f'Loss: \\t {loss_fn(x_data, y_data, feature_dict, y_vocab, eta=0.1):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44478.69569966906\n",
      "33525.67208654845\n",
      "20308.133944778292\n",
      "Loss: \t 20308.133944778292\n",
      "16107.921421833915\n",
      "Loss: \t 16107.921421833915\n",
      "41150.37349280199\n",
      "15146.290018266269\n",
      "Loss: \t 15146.290018266269\n",
      "14921.717489489878\n",
      "Loss: \t 14921.717489489878\n",
      "14463.920914536946\n",
      "Loss: \t 14463.920914536946\n",
      "14017.996594974013\n",
      "Loss: \t 14017.996594974013\n",
      "12839.139916211498\n",
      "Loss: \t 12839.139916211498\n",
      "11588.446676907115\n",
      "Loss: \t 11588.446676907115\n",
      "11474.069639827982\n",
      "10987.306013525267\n",
      "Loss: \t 10987.306013525267\n",
      "10619.654838586039\n",
      "Loss: \t 10619.654838586039\n",
      "10179.653289703536\n",
      "Loss: \t 10179.653289703536\n",
      "9788.483570045282\n",
      "Loss: \t 9788.483570045282\n",
      "9462.625389117944\n",
      "Loss: \t 9462.625389117944\n",
      "9200.22891879007\n",
      "Loss: \t 9200.22891879007\n",
      "8946.50935893688\n",
      "Loss: \t 8946.50935893688\n",
      "8585.657052324774\n",
      "Loss: \t 8585.657052324774\n",
      "8256.336089962046\n",
      "Loss: \t 8256.336089962046\n",
      "8075.0346050803155\n",
      "Loss: \t 8075.0346050803155\n",
      "7961.788518610383\n",
      "Loss: \t 7961.788518610383\n",
      "7757.598193097727\n",
      "Loss: \t 7757.598193097727\n",
      "7480.577805764386\n",
      "Loss: \t 7480.577805764386\n",
      "7254.271488708632\n",
      "Loss: \t 7254.271488708632\n",
      "7208.022770008858\n",
      "Loss: \t 7208.022770008858\n",
      "7099.367458281196\n",
      "Loss: \t 7099.367458281196\n",
      "7009.06228225132\n",
      "Loss: \t 7009.06228225132\n",
      "6838.912580049431\n",
      "Loss: \t 6838.912580049431\n",
      "6766.795617270474\n",
      "Loss: \t 6766.795617270474\n",
      "6589.271294266704\n",
      "Loss: \t 6589.271294266704\n",
      "6471.173859991914\n",
      "Loss: \t 6471.173859991914\n",
      "6339.52893546116\n",
      "Loss: \t 6339.52893546116\n",
      "6125.122439949442\n",
      "Loss: \t 6125.122439949442\n",
      "5979.9640456164125\n",
      "Loss: \t 5979.9640456164125\n",
      "5868.585035096706\n",
      "Loss: \t 5868.585035096706\n",
      "5724.798900308574\n",
      "Loss: \t 5724.798900308574\n",
      "5483.067764388208\n",
      "Loss: \t 5483.067764388208\n",
      "5318.893228256138\n",
      "Loss: \t 5318.893228256138\n",
      "6214.516710052716\n",
      "5260.76964860232\n",
      "Loss: \t 5260.76964860232\n",
      "5123.994191205202\n",
      "Loss: \t 5123.994191205202\n",
      "5027.895258174163\n",
      "Loss: \t 5027.895258174163\n",
      "4933.124951094036\n",
      "Loss: \t 4933.124951094036\n",
      "4783.738570510046\n",
      "Loss: \t 4783.738570510046\n",
      "6070.38908026476\n",
      "4695.613087125468\n",
      "Loss: \t 4695.613087125468\n",
      "4566.352463304158\n",
      "Loss: \t 4566.352463304158\n",
      "4930.441622339952\n",
      "4479.048333164566\n",
      "Loss: \t 4479.048333164566\n",
      "4403.985740231859\n",
      "Loss: \t 4403.985740231859\n",
      "4319.211412294399\n",
      "Loss: \t 4319.211412294399\n",
      "4277.41885863945\n",
      "Loss: \t 4277.41885863945\n",
      "4238.918381340398\n",
      "Loss: \t 4238.918381340398\n",
      "4186.11099377028\n",
      "Loss: \t 4186.11099377028\n",
      "4120.619102817016\n",
      "Loss: \t 4120.619102817016\n",
      "4036.7400326061115\n",
      "Loss: \t 4036.7400326061115\n",
      "3980.063130306992\n",
      "Loss: \t 3980.063130306992\n",
      "3827.0433937403077\n",
      "Loss: \t 3827.0433937403077\n",
      "3784.4097943254787\n",
      "3718.226312718351\n",
      "Loss: \t 3718.226312718351\n",
      "3648.7696859910106\n",
      "Loss: \t 3648.7696859910106\n",
      "3553.9276206671066\n",
      "Loss: \t 3553.9276206671066\n",
      "3480.572337358937\n",
      "Loss: \t 3480.572337358937\n",
      "3421.6686216578482\n",
      "Loss: \t 3421.6686216578482\n",
      "3347.967684042753\n",
      "Loss: \t 3347.967684042753\n",
      "3254.134695056705\n",
      "Loss: \t 3254.134695056705\n",
      "3201.3962862343924\n",
      "Loss: \t 3201.3962862343924\n",
      "3150.89478522921\n",
      "Loss: \t 3150.89478522921\n",
      "3096.1147014258845\n",
      "Loss: \t 3096.1147014258845\n",
      "3048.516045060551\n",
      "Loss: \t 3048.516045060551\n",
      "2995.077644544199\n",
      "Loss: \t 2995.077644544199\n",
      "2963.254061094186\n",
      "Loss: \t 2963.254061094186\n",
      "2937.173287045675\n",
      "2907.4759376830743\n",
      "Loss: \t 2907.4759376830743\n",
      "2852.882674147835\n",
      "Loss: \t 2852.882674147835\n",
      "3346.8221116954737\n",
      "2842.8472965992623\n",
      "Loss: \t 2842.8472965992623\n",
      "2823.8198599974876\n",
      "Loss: \t 2823.8198599974876\n",
      "2787.4187003488064\n",
      "Loss: \t 2787.4187003488064\n",
      "2760.1531162747015\n",
      "Loss: \t 2760.1531162747015\n",
      "2740.1917691602457\n",
      "Loss: \t 2740.1917691602457\n",
      "2727.0624247125616\n",
      "Loss: \t 2727.0624247125616\n",
      "2695.622387988631\n",
      "Loss: \t 2695.622387988631\n",
      "2679.585760027492\n",
      "Loss: \t 2679.585760027492\n",
      "2659.754503465346\n",
      "Loss: \t 2659.754503465346\n",
      "2632.49015703007\n",
      "Loss: \t 2632.49015703007\n",
      "2678.4964207849903\n",
      "2612.7303611642346\n",
      "Loss: \t 2612.7303611642346\n",
      "2627.2523587102805\n",
      "2601.582145595075\n",
      "Loss: \t 2601.582145595075\n",
      "2581.8838041305044\n",
      "Loss: \t 2581.8838041305044\n",
      "2545.3757968988007\n",
      "Loss: \t 2545.3757968988007\n",
      "2516.749814504819\n",
      "Loss: \t 2516.749814504819\n",
      "2499.582175168239\n",
      "Loss: \t 2499.582175168239\n",
      "2474.1295676959203\n",
      "Loss: \t 2474.1295676959203\n",
      "2553.3305646455788\n",
      "2465.8047936869193\n",
      "Loss: \t 2465.8047936869193\n",
      "2443.860396764284\n",
      "Loss: \t 2443.860396764284\n",
      "2421.948416263921\n",
      "Loss: \t 2421.948416263921\n",
      "2402.156264809538\n",
      "Loss: \t 2402.156264809538\n",
      "2383.0058454793216\n",
      "Loss: \t 2383.0058454793216\n",
      "2422.481359718675\n",
      "2377.625214931559\n",
      "Loss: \t 2377.625214931559\n",
      "2362.808596381679\n",
      "Loss: \t 2362.808596381679\n",
      "2355.420499558597\n",
      "Loss: \t 2355.420499558597\n",
      "2343.5206625709056\n",
      "Loss: \t 2343.5206625709056\n",
      "2338.736143518062\n",
      "Loss: \t 2338.736143518062\n",
      "2324.970904085401\n",
      "Loss: \t 2324.970904085401\n",
      "2319.664205745633\n",
      "Loss: \t 2319.664205745633\n",
      "2313.4645037395203\n",
      "Loss: \t 2313.4645037395203\n",
      "2302.216495696186\n",
      "Loss: \t 2302.216495696186\n",
      "2333.42219457211\n",
      "2298.2629120860647\n",
      "Loss: \t 2298.2629120860647\n",
      "2288.286289791533\n",
      "Loss: \t 2288.286289791533\n",
      "2282.941710518466\n",
      "Loss: \t 2282.941710518466\n",
      "2283.3220284284116\n",
      "2280.4613037046256\n",
      "Loss: \t 2280.4613037046256\n",
      "2277.117753723111\n",
      "Loss: \t 2277.117753723111\n",
      "2275.13150318341\n",
      "Loss: \t 2275.13150318341\n",
      "2269.4689944591146\n",
      "Loss: \t 2269.4689944591146\n",
      "2265.2472467300004\n",
      "Loss: \t 2265.2472467300004\n",
      "2268.0442430491066\n",
      "2261.9656459495245\n",
      "Loss: \t 2261.9656459495245\n",
      "2257.6782471159463\n",
      "Loss: \t 2257.6782471159463\n",
      "2255.7470348838287\n",
      "Loss: \t 2255.7470348838287\n",
      "2253.6021235317066\n",
      "Loss: \t 2253.6021235317066\n",
      "2249.41246876633\n",
      "Loss: \t 2249.41246876633\n",
      "2315.3102367503375\n",
      "2248.854728444073\n",
      "Loss: \t 2248.854728444073\n",
      "2245.881381684596\n",
      "Loss: \t 2245.881381684596\n",
      "2244.9906112477447\n",
      "Loss: \t 2244.9906112477447\n",
      "2243.4739248539363\n",
      "Loss: \t 2243.4739248539363\n",
      "2242.882497235877\n",
      "Loss: \t 2242.882497235877\n",
      "2241.9185490563696\n",
      "Loss: \t 2241.9185490563696\n",
      "2240.287827836396\n",
      "Loss: \t 2240.287827836396\n",
      "2239.7602096276714\n",
      "Loss: \t 2239.7602096276714\n",
      "2238.0800953675916\n",
      "Loss: \t 2238.0800953675916\n",
      "2237.1084086451774\n",
      "Loss: \t 2237.1084086451774\n",
      "2235.8066586529694\n",
      "Loss: \t 2235.8066586529694\n",
      "2235.154038967199\n",
      "Loss: \t 2235.154038967199\n",
      "2234.409400435592\n",
      "Loss: \t 2234.409400435592\n",
      "2234.704973131159\n",
      "2234.098706666562\n",
      "Loss: \t 2234.098706666562\n",
      "2233.785023789803\n",
      "Loss: \t 2233.785023789803\n",
      "2233.615141394308\n",
      "Loss: \t 2233.615141394308\n",
      "2233.203922625243\n",
      "Loss: \t 2233.203922625243\n",
      "2232.7565944996823\n",
      "Loss: \t 2232.7565944996823\n",
      "2232.406473676879\n",
      "Loss: \t 2232.406473676879\n",
      "2232.0564912473737\n",
      "Loss: \t 2232.0564912473737\n",
      "2231.4529207994055\n",
      "Loss: \t 2231.4529207994055\n",
      "2231.597025070107\n",
      "2231.0050943443216\n",
      "Loss: \t 2231.0050943443216\n",
      "2230.480572797712\n",
      "Loss: \t 2230.480572797712\n",
      "2230.319776342697\n",
      "Loss: \t 2230.319776342697\n",
      "2230.1273469018047\n",
      "Loss: \t 2230.1273469018047\n",
      "2230.140259253602\n",
      "2230.0440399162717\n",
      "Loss: \t 2230.0440399162717\n",
      "2229.969651834582\n",
      "Loss: \t 2229.969651834582\n",
      "2229.8202881872357\n",
      "Loss: \t 2229.8202881872357\n",
      "2229.7456475411473\n",
      "Loss: \t 2229.7456475411473\n",
      "2230.0789903993546\n",
      "2229.6844267685046\n",
      "Loss: \t 2229.6844267685046\n",
      "2229.5943758114026\n",
      "Loss: \t 2229.5943758114026\n",
      "2229.5555508028683\n",
      "Loss: \t 2229.5555508028683\n",
      "2229.478516936812\n",
      "Loss: \t 2229.478516936812\n",
      "2229.381508008967\n",
      "Loss: \t 2229.381508008967\n",
      "2229.427024037339\n",
      "2229.326597534403\n",
      "Loss: \t 2229.326597534403\n",
      "2229.2296727390903\n",
      "Loss: \t 2229.2296727390903\n",
      "2229.2159265961736\n",
      "Loss: \t 2229.2159265961736\n",
      "2229.165183586555\n",
      "Loss: \t 2229.165183586555\n",
      "2229.139579812404\n",
      "Loss: \t 2229.139579812404\n",
      "2229.1028575336495\n",
      "Loss: \t 2229.1028575336495\n",
      "2229.066602847267\n",
      "Loss: \t 2229.066602847267\n",
      "2229.0632789258657\n",
      "2229.042870581291\n",
      "Loss: \t 2229.042870581291\n",
      "2228.9997409652824\n",
      "Loss: \t 2228.9997409652824\n",
      "2228.9379404338624\n",
      "Loss: \t 2228.9379404338624\n"
     ]
    }
   ],
   "source": [
    "init_weight = np.random.random(len(feature_dict))\n",
    "feature_dict = numpy_to_dict(init_weight, feature_dict)\n",
    "result = fmin_l_bfgs_b(get_loss_grad, \n",
    "                       init_weight, \n",
    "                       args=(x_data, y_data, feature_dict, y_vocab), \n",
    "                       pgtol=0.01, \n",
    "                       callback=callbackF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (ii): write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "inferece(partial_dir/'dev.in', y_vocab, feature_dict, partial_dir/'dev.p4.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (iii) Structured Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsumexp = max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
