{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from conlleval import evaluate\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import json\n",
    "ninf = -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def read_data(path, column=0):\n",
    "    \"\"\"column=0 means input sequence, column=1 means label\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    data = []\n",
    "    sample = []\n",
    "    \n",
    "    for line in lines:\n",
    "        formatted_line = line.strip()\n",
    "        \n",
    "        if len(formatted_line) > 0:\n",
    "            split_data = formatted_line.split(\" \")\n",
    "            sample.append(split_data[column])\n",
    "\n",
    "        else:\n",
    "            data.append(sample)\n",
    "            sample = []\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "full_dir = Path('full')\n",
    "partial_dir = Path('partial')\n",
    "save_dir = Path('save')\n",
    "x_data, y_data = read_data(partial_dir/'train', 0), read_data(partial_dir/'train', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 700)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of instances in the dataset\n",
    "len(x_data), len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y labels\n",
    "y_vocab = sorted(list(set([oo for o in y_data for oo in o]))); y_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4068"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x vocab\n",
    "x_vocab = list(set([oo for o in x_data for oo in o])); len(x_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part I (i): Emission scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def calc_e(x_data, y_data, x_vocab, y_vocab):\n",
    "    count_emission = Counter([(x,y) for x_instance, y_instance in zip(x_data, y_data) for x, y in zip(x_instance, y_instance)])\n",
    "    count_label = Counter([oo for o in y_data for oo in o])\n",
    "    \n",
    "    e_score = {}\n",
    "    for y in y_vocab:\n",
    "        for x in x_vocab:\n",
    "            feature = f\"emission:{y}+{x}\"\n",
    "            \n",
    "            if (x,y) not in count_emission:\n",
    "                e_score[feature] = ninf\n",
    "            else:\n",
    "                score = np.log(count_emission[(x,y)]  /  count_label[y])\n",
    "                e_score[feature] = score\n",
    "    \n",
    "    return e_score\n",
    "\n",
    "\n",
    "emission_dict = calc_e(x_data, y_data, x_vocab, y_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part I (ii): Transition scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def calc_t(y_data, y_vocab):\n",
    "    count_transition = Counter([ (y_prev, y) for y_instance in y_data for y_prev, y in zip(['START'] + y_instance, y_instance + ['STOP'])])\n",
    "    count_label = Counter([y for y_instance in y_data for y in ['START'] + y_instance])\n",
    "    \n",
    "    f_score = {}\n",
    "    for y_prev in ['START'] + y_vocab:\n",
    "        for y in y_vocab + ['STOP']:\n",
    "            feature = f\"transition:{y_prev}+{y}\"\n",
    "            \n",
    "            if (y_prev,y) not in count_transition:\n",
    "                f_score[feature] = ninf\n",
    "            else:\n",
    "                score = np.log(count_transition[(y_prev,y)]  /  count_label[y_prev])\n",
    "                f_score[feature] = score\n",
    "    \n",
    "    return f_score\n",
    "\n",
    "transition_dict = calc_t(y_data, y_vocab)\n",
    "feature_dict = {**transition_dict, **emission_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part II (i): Compute Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-139.57175855522826"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_score(x_instance, y_instance, feature_dict):\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    for x, y in zip(x_instance, y_instance): feature_count[f\"emission:{y}+{x}\"] += 1\n",
    "    \n",
    "    for y_prev, y in zip(['START'] + y_instance, y_instance + ['STOP']):\n",
    "        feature_count[f\"transition:{y_prev}+{y}\"] += 1\n",
    "        \n",
    "    score = sum([feature_dict[feat]*count for feat, count in feature_count.items()])\n",
    "    return score\n",
    "\n",
    "x_instance = \"This is the second U.N.-Congolese offensive against militias in the region since the DRC   's constitutional referendum a week ago .\".split()\n",
    "y_instance = 'O    O  O   O      O              O         O       O        O  O   O      O     O   B-geo O  O              O          O O    O   O'.split()\n",
    "compute_score(x_instance, y_instance, feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part II (ii): Viterbi decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  O O O O O O O O O O O O O B-geo O O O O O O O\n",
      "y_labl:  O O O O O O O O O O O O O B-geo O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "def viterbi(x_instance, y_vocab, feature_dict):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.full( (n,d), -np.inf) # initialize to be very negative\n",
    "    bp = np.full( (n,d), 0, dtype=np.int)\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:START+{y}\",  ninf)\n",
    "        e_score = feature_dict.get( f\"emission:{y}+{x_instance[0]}\",  ninf)\n",
    "        scores[0, i] = t_score + e_score\n",
    "        \n",
    "    for i in range(1, n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            for y_prev_i, y_prev in enumerate(y_vocab):\n",
    "                t_score = feature_dict.get( f\"transition:{y_prev}+{y}\", ninf)\n",
    "                e_score = feature_dict.get( f\"emission:{y}+{x_instance[i]}\", ninf)\n",
    "                score = t_score + e_score + scores[i-1, y_prev_i]\n",
    "                if score > scores[i, y_i]:\n",
    "                    scores[i, y_i] = score\n",
    "                    bp[i, y_i] = y_prev_i\n",
    "    \n",
    "    final_score, final_bp = ninf, 0\n",
    "    for i, y_prev in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:{y_prev}+STOP\", ninf)\n",
    "        score = t_score + scores[n-1, i]\n",
    "        if score > final_score: \n",
    "            final_score = score\n",
    "            final_bp = i\n",
    "    decoded_sequence = [ y_vocab[final_bp], ]\n",
    "    for i in range(n-1, 0, -1):\n",
    "        final_bp = bp[i, final_bp]\n",
    "        decoded_sequence = [ y_vocab[final_bp] ] + decoded_sequence\n",
    "        \n",
    "    return decoded_sequence\n",
    "\n",
    "print(\"y_pred: \", \" \".join(viterbi(x_instance, y_vocab, feature_dict)))\n",
    "print(\"y_labl: \", \" \".join(y_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 49.442 \t rec: 56.356 \t f1 52.673\n"
     ]
    }
   ],
   "source": [
    "def inference(in_file_path, y_vocab, feature_dict, out_file_path):\n",
    "    x_data = read_data(in_file_path, column=0)\n",
    "    y_preds = []\n",
    "    with open(out_file_path, 'w') as f:\n",
    "        for x_instance in x_data:\n",
    "            pred = viterbi(x_instance, y_vocab, feature_dict)\n",
    "            for word, label in zip(x_instance, pred): f.write(f\"{word} {label} \\n\")\n",
    "            f.write('\\n')\n",
    "            y_preds.append(pred)\n",
    "    return y_preds\n",
    "\n",
    "y_preds = inference(partial_dir/'dev.in', y_vocab, feature_dict, partial_dir/'dev.p2.out')\n",
    "y_label = read_data(partial_dir/'dev.out', column=1)\n",
    "\n",
    "y_label = [oo for o in y_label for oo in o+['O']]\n",
    "y_preds = [oo for o in y_preds for oo in o+['O']]\n",
    "\n",
    "prec, rec, f1 = evaluate(y_label, y_preds, verbose=False)\n",
    "print(f'precision: {prec:.3f} \\t rec: {rec:.3f} \\t f1 {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part III (i): CRF loss\n",
    "refer to [machine learning slides](https://drive.google.com/file/d/1RfPcnQigx4jdLtnTjjjI1Jgd2UufexhV/view?usp=sharing) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3302877199753311"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logsumexp(a):\n",
    "    b = a.max()\n",
    "    return  b + np.log( (np.exp(a-b)).sum() )\n",
    "\n",
    "def forward(x_instance, y_vocab, feature_dict):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.zeros( (n,d) )\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:START+{y}\", ninf)\n",
    "        scores[0, i] = t_score\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            temp = []\n",
    "            for y_prev_i, y_prev in enumerate(y_vocab):\n",
    "                t_score = feature_dict.get( f\"transition:{y_prev}+{y}\", ninf)\n",
    "                e_score = feature_dict.get( f\"emission:{y_prev}+{x_instance[i-1]}\", ninf)\n",
    "                temp.append(e_score + t_score + scores[i-1, y_prev_i])\n",
    "            scores[i, y_i] = logsumexp(np.array(temp))\n",
    "    \n",
    "    temp = []\n",
    "    for i, y_prev in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:{y_prev}+STOP\", ninf)\n",
    "        e_score = feature_dict.get( f\"emission:{y_prev}+{x_instance[-1]}\", ninf)\n",
    "        temp.append(e_score + t_score + scores[-1, i])\n",
    "    alpha = logsumexp(np.array(temp))\n",
    "    \n",
    "    return scores, alpha\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn_instance(x_instance, y_instance, feature_dict, y_vocab):\n",
    "    first_term = compute_score(x_instance, y_instance, feature_dict)\n",
    "    _, forward_score = forward(x_instance, y_vocab, feature_dict)\n",
    "    return forward_score - first_term\n",
    "\n",
    "\n",
    "loss_fn_instance(x_instance, y_instance, feature_dict, y_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part III (ii): forward backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def backward(x_instance, y_vocab, feature_dict, aggreg_fn=logsumexp):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.zeros( (n,d) )\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:{y}+STOP\", ninf)\n",
    "        e_score = feature_dict.get( f\"emission:{y}+{x_instance[-1]}\", ninf)\n",
    "        scores[-1, i] = t_score + e_score\n",
    "        \n",
    "    for i in range(n-1, 0, -1):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            temp = []\n",
    "            for y_next_i, y_next in enumerate(y_vocab):\n",
    "                t_score = feature_dict.get( f\"transition:{y}+{y_next}\", ninf)\n",
    "                e_score = feature_dict.get( f\"emission:{y}+{x_instance[i-1]}\")\n",
    "                temp.append(e_score + t_score + scores[i, y_next_i])\n",
    "            scores[i-1, y_i] = aggreg_fn(np.array(temp))\n",
    "            \n",
    "    temp = []\n",
    "    for i, y_next in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:START+{y_next}\")\n",
    "        temp.append(t_score + scores[0, i])\n",
    "    beta = aggreg_fn(np.array(temp))\n",
    "    \n",
    "    return scores, beta\n",
    "\n",
    "\n",
    "\n",
    "def forward_backward(x_instance, y_vocab, feature_dict):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    f_scores, alpha = forward(x_instance, y_vocab, feature_dict)\n",
    "    b_scores, beta = backward(x_instance, y_vocab, feature_dict)\n",
    "    \n",
    "    feature_expected_count = defaultdict(float)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            e_feature = f\"emission:{y}+{x_instance[i]}\"\n",
    "            feature_expected_count[e_feature] += np.exp(f_scores[i, y_i] + b_scores[i, y_i] - alpha)\n",
    "            \n",
    "    for i, y_next in enumerate(y_vocab):\n",
    "        t_feature = f\"transition:START+{y_next}\"\n",
    "        feature_expected_count[t_feature] += np.exp(f_scores[0, i] + b_scores[0, i] - alpha)\n",
    "        \n",
    "        t_feature = f\"transition:{y_next}+STOP\"\n",
    "        feature_expected_count[t_feature] += np.exp(f_scores[-1, i] + b_scores[-1, i] - alpha)\n",
    "        \n",
    "    for y_i, y in enumerate(y_vocab):\n",
    "        for y_next_i, y_next in enumerate(y_vocab):\n",
    "            t_feature = f\"transition:{y}+{y_next}\"\n",
    "            t_score = feature_dict.get(t_feature, ninf)\n",
    "            total = 0\n",
    "            for i in range(n-1):\n",
    "                e_score = feature_dict.get(f\"emission:{y}+{x_instance[i]}\", ninf)\n",
    "                total += np.exp(f_scores[i, y_i] + b_scores[i+1, y_next_i] + t_score + e_score - alpha)\n",
    "            feature_expected_count[t_feature] = total\n",
    "            \n",
    "    return feature_expected_count\n",
    "\n",
    "def get_feature_count(x_instance, y_instance, feature_dict):\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    for x, y in zip(x_instance, y_instance): feature_count[f\"emission:{y}+{x}\"] += 1\n",
    "    \n",
    "    for y_prev, y in zip(['START'] + y_instance, y_instance + ['STOP']):\n",
    "        feature_count[f\"transition:{y_prev}+{y}\"] += 1\n",
    "    \n",
    "    return feature_count\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Part IV (i): gradient and training with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def gradient_fn(x_data, y_data, feature_dict, y_vocab, eta=0.1):\n",
    "    feature_grad = defaultdict(float)\n",
    "    \n",
    "    for x_instance, y_instance in zip(x_data, y_data):\n",
    "        feature_expected_counts = forward_backward(x_instance, y_vocab, feature_dict)\n",
    "        feature_actual_counts = get_feature_count(x_instance, y_instance, feature_dict)\n",
    "        for k, v in feature_expected_counts.items(): feature_grad[k] += v\n",
    "        for k, v in feature_actual_counts.items(): feature_grad[k] -= v    \n",
    "    \n",
    "    if eta > 0: \n",
    "        for k, v in feature_dict.items(): feature_grad[k] += 2*eta*v\n",
    "    \n",
    "    return feature_grad\n",
    "        \n",
    "    \n",
    "def loss_fn(x_data, y_data, feature_dict, y_vocab, eta=0):\n",
    "    loss = 0\n",
    "    for x_instance, y_instance in zip(x_data, y_data):\n",
    "        loss += loss_fn_instance(x_instance, y_instance, feature_dict, y_vocab) \n",
    "    reg_loss = eta * sum([o**2 for o in feature_dict.values()]) if eta > 0 else 0\n",
    "    return loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      emission:O+the passed gradient checking!\n",
      "  transition:START+O passed gradient checking!\n",
      "      transition:O+O passed gradient checking!\n",
      "transition:I-per+I-per passed gradient checking!\n"
     ]
    }
   ],
   "source": [
    "# Gradient verification\n",
    "feature_key_checks = ['emission:O+the', 'transition:START+O', 'transition:O+O', 'transition:I-per+I-per']\n",
    "feature_gradients = gradient_fn(x_data, y_data, feature_dict, y_vocab, eta=0)\n",
    "loss1 = loss_fn(x_data, y_data, feature_dict, y_vocab, eta=0)\n",
    "delta = 1e-6\n",
    "\n",
    "for feat_k in feature_key_checks:\n",
    "    new_feature_dict = feature_dict.copy()\n",
    "    new_feature_dict[feat_k] += delta\n",
    "    loss2 = loss_fn(x_data, y_data, new_feature_dict, y_vocab, eta=0)\n",
    "    numerical_grad = (loss2 - loss1) / delta\n",
    "    analytic_grad = feature_gradients[feat_k]\n",
    "    if abs(numerical_grad - analytic_grad) / max(abs(numerical_grad), 1e-8) < 1e-5: \n",
    "        print(f'{feat_k:>40} passed gradient checking!')\n",
    "    else:\n",
    "        print(f'{feat_k:>40} didnot pass gradient checking!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \t 18333.7955\n",
      "Loss: \t 14132.2391\n",
      "Loss: \t 13066.3711\n",
      "Loss: \t 12663.9164\n",
      "Loss: \t 12303.2443\n",
      "Loss: \t 11099.0803\n",
      "Loss: \t 10489.4525\n",
      "Loss: \t 9614.8077\n",
      "Loss: \t 9067.4347\n",
      "Loss: \t 8394.3281\n",
      "Loss: \t 8004.4294\n",
      "Loss: \t 7673.0057\n",
      "Loss: \t 7275.4810\n",
      "Loss: \t 6911.1102\n",
      "Loss: \t 6710.6507\n",
      "Loss: \t 6469.9394\n",
      "Loss: \t 6113.1164\n",
      "Loss: \t 6011.9983\n",
      "Loss: \t 5886.4793\n",
      "Loss: \t 5688.9170\n",
      "Loss: \t 5620.4006\n",
      "Loss: \t 5526.2114\n",
      "Loss: \t 5410.1797\n",
      "Loss: \t 5263.5436\n",
      "Loss: \t 5010.8256\n",
      "Loss: \t 4891.2793\n",
      "Loss: \t 4772.6344\n",
      "Loss: \t 4602.0067\n",
      "Loss: \t 4504.9576\n",
      "Loss: \t 4438.6535\n",
      "Loss: \t 4382.5379\n",
      "Loss: \t 4312.2697\n",
      "Loss: \t 4146.8862\n",
      "Loss: \t 3931.2384\n",
      "Loss: \t 3877.3722\n",
      "Loss: \t 3638.9999\n",
      "Loss: \t 3547.3763\n",
      "Loss: \t 3424.4592\n",
      "Loss: \t 3289.2315\n",
      "Loss: \t 3263.2814\n",
      "Loss: \t 3149.6003\n",
      "Loss: \t 3129.3778\n",
      "Loss: \t 3088.5620\n",
      "Loss: \t 3031.8828\n",
      "Loss: \t 2977.3570\n",
      "Loss: \t 2917.4135\n",
      "Loss: \t 2890.3171\n",
      "Loss: \t 2833.4111\n",
      "Loss: \t 2783.4258\n",
      "Loss: \t 2686.1813\n",
      "Loss: \t 2643.4129\n",
      "Loss: \t 2600.7354\n",
      "Loss: \t 2583.4919\n",
      "Loss: \t 2549.4932\n",
      "Loss: \t 2524.9203\n",
      "Loss: \t 2502.2955\n",
      "Loss: \t 2495.3878\n",
      "Loss: \t 2489.2714\n",
      "Loss: \t 2473.9431\n",
      "Loss: \t 2450.0810\n",
      "Loss: \t 2422.6368\n",
      "Loss: \t 2398.3073\n",
      "Loss: \t 2384.3053\n",
      "Loss: \t 2377.2105\n",
      "Loss: \t 2363.4270\n",
      "Loss: \t 2350.8484\n",
      "Loss: \t 2339.7542\n",
      "Loss: \t 2333.7093\n",
      "Loss: \t 2324.2190\n",
      "Loss: \t 2316.9614\n",
      "Loss: \t 2313.5039\n",
      "Loss: \t 2309.5335\n",
      "Loss: \t 2307.8828\n",
      "Loss: \t 2301.3470\n",
      "Loss: \t 2294.1057\n",
      "Loss: \t 2283.8769\n",
      "Loss: \t 2280.3981\n",
      "Loss: \t 2274.8810\n",
      "Loss: \t 2271.5517\n",
      "Loss: \t 2268.9940\n",
      "Loss: \t 2267.8335\n",
      "Loss: \t 2265.7117\n",
      "Loss: \t 2263.7747\n",
      "Loss: \t 2262.8404\n",
      "Loss: \t 2260.4822\n",
      "Loss: \t 2259.0091\n",
      "Loss: \t 2255.6376\n",
      "Loss: \t 2253.7486\n",
      "Loss: \t 2252.5612\n",
      "Loss: \t 2251.1748\n",
      "Loss: \t 2248.1788\n",
      "Loss: \t 2247.5830\n",
      "Loss: \t 2245.5654\n",
      "Loss: \t 2244.8452\n",
      "Loss: \t 2244.0129\n",
      "Loss: \t 2242.3143\n",
      "Loss: \t 2240.6339\n",
      "Loss: \t 2239.0969\n",
      "Loss: \t 2238.5592\n",
      "Loss: \t 2237.7739\n",
      "Loss: \t 2237.3731\n",
      "Loss: \t 2237.0004\n",
      "Loss: \t 2236.5396\n",
      "Loss: \t 2235.7711\n",
      "Loss: \t 2235.6152\n",
      "Loss: \t 2234.9615\n",
      "Loss: \t 2234.5293\n",
      "Loss: \t 2233.8771\n",
      "Loss: \t 2233.1602\n",
      "Loss: \t 2233.0260\n",
      "Loss: \t 2232.7104\n",
      "Loss: \t 2232.4093\n",
      "Loss: \t 2232.1178\n",
      "Loss: \t 2231.7976\n",
      "Loss: \t 2231.6077\n",
      "Loss: \t 2231.2559\n",
      "Loss: \t 2231.1797\n",
      "Loss: \t 2231.0717\n",
      "Loss: \t 2230.8816\n",
      "Loss: \t 2230.5278\n",
      "Loss: \t 2230.4104\n",
      "Loss: \t 2230.2654\n",
      "Loss: \t 2230.1896\n",
      "Loss: \t 2230.1254\n",
      "Loss: \t 2230.0949\n",
      "Loss: \t 2230.0348\n",
      "Loss: \t 2229.9479\n",
      "Loss: \t 2229.8907\n",
      "Loss: \t 2229.8442\n",
      "Loss: \t 2229.7602\n",
      "Loss: \t 2229.6432\n",
      "Loss: \t 2229.4902\n",
      "Loss: \t 2229.3438\n",
      "Loss: \t 2229.3107\n",
      "Loss: \t 2229.2153\n",
      "Loss: \t 2229.1924\n",
      "Loss: \t 2229.1299\n",
      "Loss: \t 2229.0838\n",
      "Loss: \t 2229.0369\n",
      "Loss: \t 2229.0219\n",
      "Loss: \t 2228.9987\n",
      "Loss: \t 2228.9950\n",
      "Loss: \t 2228.9663\n",
      "Loss: \t 2228.9556\n",
      "Loss: \t 2228.9346\n",
      "Loss: \t 2228.9000\n",
      "Loss: \t 2228.8802\n",
      "Loss: \t 2228.8474\n",
      "Loss: \t 2228.8278\n",
      "Loss: \t 2228.8128\n",
      "Loss: \t 2228.8048\n",
      "Loss: \t 2228.7845\n",
      "Loss: \t 2228.7662\n",
      "Loss: \t 2228.7509\n",
      "Loss: \t 2228.7365\n",
      "Loss: \t 2228.7296\n",
      "Loss: \t 2228.7051\n",
      "Loss: \t 2228.6981\n",
      "Loss: \t 2228.6847\n",
      "Loss: \t 2228.6779\n",
      "Loss: \t 2228.6640\n",
      "Loss: \t 2228.6496\n",
      "Loss: \t 2228.6362\n",
      "Loss: \t 2228.6342\n",
      "Loss: \t 2228.6308\n",
      "Loss: \t 2228.6273\n",
      "Loss: \t 2228.6192\n",
      "Loss: \t 2228.6082\n",
      "Loss: \t 2228.6036\n",
      "Loss: \t 2228.5870\n",
      "Loss: \t 2228.5838\n",
      "Loss: \t 2228.5806\n",
      "Loss: \t 2228.5778\n",
      "Loss: \t 2228.5735\n",
      "Loss: \t 2228.5719\n",
      "Loss: \t 2228.5692\n",
      "Loss: \t 2228.5684\n",
      "Loss: \t 2228.5673\n",
      "Loss: \t 2228.5623\n",
      "Loss: \t 2228.5580\n",
      "Loss: \t 2228.5531\n",
      "Loss: \t 2228.5505\n",
      "Loss: \t 2228.5480\n",
      "Loss: \t 2228.5448\n",
      "Loss: \t 2228.5420\n",
      "Loss: \t 2228.5413\n",
      "Loss: \t 2228.5402\n",
      "Loss: \t 2228.5399\n",
      "Loss: \t 2228.5392\n",
      "Loss: \t 2228.5383\n",
      "Loss: \t 2228.5370\n",
      "Loss: \t 2228.5367\n",
      "Loss: \t 2228.5347\n",
      "Loss: \t 2228.5340\n",
      "Loss: \t 2228.5334\n",
      "Loss: \t 2228.5326\n",
      "Loss: \t 2228.5307\n",
      "Loss: \t 2228.5303\n",
      "Loss: \t 2228.5294\n",
      "Loss: \t 2228.5288\n",
      "Loss: \t 2228.5282\n",
      "Loss: \t 2228.5276\n",
      "Loss: \t 2228.5266\n",
      "Loss: \t 2228.5263\n",
      "Loss: \t 2228.5258\n",
      "Loss: \t 2228.5255\n",
      "Loss: \t 2228.5251\n",
      "Loss: \t 2228.5246\n",
      "Loss: \t 2228.5241\n",
      "Loss: \t 2228.5240\n",
      "Loss: \t 2228.5237\n",
      "Loss: \t 2228.5236\n",
      "Loss: \t 2228.5235\n",
      "Loss: \t 2228.5232\n",
      "Loss: \t 2228.5229\n",
      "Loss: \t 2228.5228\n",
      "Loss: \t 2228.5226\n",
      "Loss: \t 2228.5224\n",
      "Loss: \t 2228.5223\n",
      "Loss: \t 2228.5221\n",
      "Loss: \t 2228.5220\n",
      "Loss: \t 2228.5218\n",
      "Loss: \t 2228.5217\n",
      "Loss: \t 2228.5216\n",
      "Loss: \t 2228.5216\n",
      "Loss: \t 2228.5215\n",
      "Loss: \t 2228.5214\n",
      "Loss: \t 2228.5213\n",
      "Loss: \t 2228.5212\n",
      "Loss: \t 2228.5212\n",
      "Loss: \t 2228.5211\n",
      "Loss: \t 2228.5211\n",
      "Loss: \t 2228.5211\n",
      "Loss: \t 2228.5210\n",
      "Loss: \t 2228.5210\n",
      "Loss: \t 2228.5210\n",
      "Loss: \t 2228.5209\n",
      "Loss: \t 2228.5209\n",
      "Loss: \t 2228.5208\n",
      "Loss: \t 2228.5208\n",
      "Loss: \t 2228.5208\n",
      "Loss: \t 2228.5207\n",
      "Loss: \t 2228.5207\n",
      "Loss: \t 2228.5207\n",
      "Loss: \t 2228.5207\n",
      "Loss: \t 2228.5207\n",
      "Loss: \t 2228.5206\n",
      "Loss: \t 2228.5206\n",
      "Loss: \t 2228.5206\n",
      "Loss: \t 2228.5206\n",
      "Loss: \t 2228.5206\n",
      "Loss: \t 2228.5206\n",
      "Loss: \t 2228.5206\n",
      "Loss: \t 2228.5205\n",
      "Loss: \t 2228.5205\n",
      "Loss: \t 2228.5205\n",
      "Loss: \t 2228.5205\n"
     ]
    }
   ],
   "source": [
    "# Helper function\n",
    "def numpy_to_dict(weight, feature_dict):\n",
    "    for i,k in enumerate(feature_dict.keys()):\n",
    "        feature_dict[k] = weight[i]\n",
    "    return feature_dict\n",
    "\n",
    "def dict_to_numpy(grads, feature_dict):\n",
    "    np_grads = np.zeros(len(feature_dict))\n",
    "    for i, k in enumerate(feature_dict.keys()):\n",
    "        np_grads[i] = grads[k]\n",
    "    return np_grads\n",
    "\n",
    "def get_loss_grad(weight, *args):\n",
    "    x_data, y_data, feature_dict, y_vocab = args\n",
    "    feature_dict = numpy_to_dict(weight, feature_dict)\n",
    "    loss = loss_fn(x_data, y_data, feature_dict, y_vocab, eta=0.1)\n",
    "    grads = gradient_fn(x_data, y_data, feature_dict, y_vocab, eta=0.1)\n",
    "    grads = dict_to_numpy(grads, feature_dict)\n",
    "    return loss, grads\n",
    "\n",
    "def callbackF(weight): print(f'Loss: \\t {loss_fn(x_data, y_data, feature_dict, y_vocab, eta=0.1):.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# Initialization\n",
    "init_weight = np.zeros(len(feature_dict))\n",
    "feature_dict = numpy_to_dict(init_weight, feature_dict)\n",
    "\n",
    "\n",
    "# Training\n",
    "result = fmin_l_bfgs_b( \n",
    "    get_loss_grad, init_weight, pgtol=0.01, callback=callbackF,\n",
    "    args=(x_data, y_data, feature_dict, y_vocab) \n",
    ")\n",
    "\n",
    "# Save weights\n",
    "feature_dict = numpy_to_dict(result[0], feature_dict)\n",
    "weight_name = save_dir/'partial-part4-1.json'\n",
    "with open(weight_name, 'w') as f: json.dump(feature_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (ii): write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 54.321 \t rec: 55.932 \t f1 55.115\n"
     ]
    }
   ],
   "source": [
    "weight_name = save_dir/'partial-part4-1.json'\n",
    "with open(weight_name) as f: feature_dict = json.load(f)\n",
    "\n",
    "y_preds = inference(partial_dir/'dev.in', y_vocab, feature_dict, partial_dir/'dev.p4.out')\n",
    "y_label = read_data(partial_dir/'dev.out', column=1)\n",
    "\n",
    "y_label = [oo for o in y_label for oo in o+['O']]\n",
    "y_preds = [oo for o in y_preds for oo in o+['O']]\n",
    "\n",
    "prec, rec, f1 = evaluate(y_label, y_preds, verbose=False)\n",
    "print(f'precision: {prec:.3f} \\t rec: {rec:.3f} \\t f1 {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (iii) Structured Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsumexp = max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
