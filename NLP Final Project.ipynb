{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import conlleval\n",
    "from pathlib import Path\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import copy\n",
    "\n",
    "save_dir = Path('save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_train(filename):\n",
    "    file_object = open(filename, 'r')\n",
    "    X = []\n",
    "    Y1 = []\n",
    "    Y2 = []\n",
    "\n",
    "    x_sent = []\n",
    "    y1_sent = []\n",
    "    y2_sent = []\n",
    "    try:\n",
    "        for line in file_object:\n",
    "            if len(line) != 1:\n",
    "                word, pos, ner = line.split()\n",
    "                x_sent.append(word)\n",
    "                y1_sent.append(pos)\n",
    "                y2_sent.append(ner)\n",
    "\n",
    "            else:\n",
    "                X.append(x_sent)\n",
    "                x_sent = []\n",
    "\n",
    "                Y1.append(y1_sent)\n",
    "                y1_sent = []\n",
    "\n",
    "                Y2.append(y2_sent)\n",
    "                y2_sent = []\n",
    "    finally:\n",
    "        file_object.close()\n",
    "    return X, Y1, Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X length: 700, Y length: 700\n"
     ]
    }
   ],
   "source": [
    "partial_train = \"../data/partial/train\"\n",
    "full_train = \"../data/full/train\"\n",
    "\n",
    "ninf = -1e5\n",
    "\n",
    "X, Y_POS, Y_NER = get_full_train(full_train)\n",
    "print(\"X length: {}, Y length: {}\".format(len(X), len(Y_NER)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set(mlist):\n",
    "    '''\n",
    "    Assmue mlist is a 2-d list with STRING elements\n",
    "    '''\n",
    "    output = set()\n",
    "    for line in mlist:\n",
    "        output.update(line)\n",
    "    return sorted(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission(Y, X, Y_NER_set, X_set, smooth=0.01):\n",
    "    '''\n",
    "    emission: Y -> X\n",
    "    '''\n",
    "    count_y = dict()\n",
    "    count_yx = dict()\n",
    "    \n",
    "    for y_token in Y_NER_set:\n",
    "        count_y[y_token] = len(X_set) * smooth\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        for y_token, x_token in zip(Y[i], X[i]):\n",
    "            if (y_token, x_token) in count_yx:\n",
    "                count_y[y_token] += 1\n",
    "                count_yx[(y_token, x_token)] += 1\n",
    "            else:\n",
    "                count_yx[(y_token, x_token)] = 1\n",
    "        \n",
    "    emission_prob = dict()\n",
    "\n",
    "    for y_token in Y_NER_set:\n",
    "        for x_token in X_set:\n",
    "            key = \"emission:\" + y_token + \"+\" + x_token\n",
    "            if (y_token, x_token) in count_yx:\n",
    "                emission_prob[key] = np.log(count_yx[(y_token, x_token)] / count_y[y_token])\n",
    "            else:\n",
    "#                 emission_prob[key] = np.log(smooth / count_y[y_token])\n",
    "                emission_prob[key] = -1e8\n",
    "    return emission_prob\n",
    "\n",
    "def calc_e(x_data, y_data, x_vocab, y_vocab):\n",
    "    count_emission = Counter([(x,y) for x_instance, y_instance in zip(x_data, y_data) for x, y in zip(x_instance, y_instance)])\n",
    "    count_label = Counter([oo for o in y_data for oo in o])\n",
    "    \n",
    "    e_score = {}\n",
    "    for y in y_vocab:\n",
    "        for x in x_vocab:\n",
    "            feature = f\"emission:{y}+{x}\"\n",
    "            \n",
    "            if (x,y) not in count_emission:\n",
    "                e_score[feature] = ninf\n",
    "            else:\n",
    "                score = np.log(count_emission[(x,y)]  /  count_label[y])\n",
    "                e_score[feature] = score\n",
    "    \n",
    "    return e_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(Y, Y_NER_set, smooth=0.01):\n",
    "    count_y = dict()\n",
    "    count_yy = dict()\n",
    "\n",
    "    for y_token in Y_NER_set+['START']:\n",
    "        count_y[y_token] = (len(Y_NER_set) + 1) * smooth\n",
    "            \n",
    "    for y_sent in Y:\n",
    "        y_p = ['START'] + y_sent + ['STOP']\n",
    "        for i in range(len(y_p)-1):\n",
    "            if (y_p[i], y_p[i+1]) in count_yy:\n",
    "                count_y[y_p[i]] += 1\n",
    "                count_yy[(y_p[i], y_p[i+1])] += 1\n",
    "            else:\n",
    "                count_yy[(y_p[i], y_p[i+1])] = 1\n",
    "    \n",
    "    transition_prob = dict()\n",
    "    \n",
    "    for y1 in ['START']+Y_NER_set:\n",
    "        for y2 in Y_NER_set+['STOP']:\n",
    "            if y1 == 'START'and y2 == 'STOP':\n",
    "                break\n",
    "            key = \"transition:\" + y1 + \"+\" + y2\n",
    "            if (y1, y2) in count_yy:\n",
    "                transition_prob[key] = np.log(count_yy[(y1, y2)] / count_y[y1])\n",
    "            else:\n",
    "#                 transition_prob[key] = np.log(smooth / count_y[y1])\n",
    "                transition_prob[key] = -1e8\n",
    "                \n",
    "    return transition_prob\n",
    "\n",
    "def calc_t(y_data, y_vocab):\n",
    "    count_transition = Counter([ (y_prev, y) for y_instance in y_data for y_prev, y in zip(['START'] + y_instance, y_instance + ['STOP'])])\n",
    "    count_label = Counter([y for y_instance in y_data for y in ['START'] + y_instance])\n",
    "    \n",
    "    f_score = {}\n",
    "    for y_prev in ['START'] + y_vocab:\n",
    "        for y in y_vocab + ['STOP']:\n",
    "            feature = f\"transition:{y_prev}+{y}\"\n",
    "            \n",
    "            if (y_prev,y) not in count_transition:\n",
    "                f_score[feature] = ninf\n",
    "            else:\n",
    "                score = np.log(count_transition[(y_prev,y)]  /  count_label[y_prev])\n",
    "                f_score[feature] = score\n",
    "    \n",
    "    return f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set = get_set(X)\n",
    "Y_NER_set = get_set(Y_NER)\n",
    "\n",
    "# e = emission(Y_NER, X, Y_NER_set, X_set, smooth=0.001)\n",
    "# t = transition(Y_NER, Y_NER_set, smooth=0.001)\n",
    "e = calc_e(X, Y_NER, X_set, Y_NER_set)\n",
    "t = calc_t(Y_NER, Y_NER_set)\n",
    "\n",
    "W = {**e, **t}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(x_sent, y_sent, f_weights):\n",
    "\n",
    "    score = 0\n",
    "    y_sent_p = ['START'] + y_sent + ['STOP']\n",
    "    x_sent_p = ['START'] + x_sent\n",
    "    for i in range(1, len(x_sent_p)):\n",
    "        word, tag1, tag2 = x_sent_p[i], y_sent_p[i-1], y_sent_p[i]\n",
    "        t_key = \"transition:\" + tag1 + \"+\" + tag2\n",
    "        score += f_weights[t_key]\n",
    "        e_key = \"emission:\" + tag2 + \"+\" + word\n",
    "        score += f_weights[e_key]\n",
    "        if i == len(x_sent_p) - 1:\n",
    "            tag1 = y_sent_p[i]\n",
    "            tag2 = y_sent_p[i+1]\n",
    "            t_key = \"transition:\" + tag1 + \"+\" + tag2\n",
    "            score += t[t_key]\n",
    "        \n",
    "    return score\n",
    "\n",
    "# def compute_score(x_instance, y_instance, feature_dict):\n",
    "#     feature_count = defaultdict(int)\n",
    "    \n",
    "#     for x, y in zip(x_instance, y_instance): feature_count[f\"emission:{y}+{x}\"] += 1\n",
    "    \n",
    "#     for y_prev, y in zip(['START'] + y_instance, y_instance + ['STOP']):\n",
    "#         feature_count[f\"transition:{y_prev}+{y}\"] += 1\n",
    "        \n",
    "#     score = sum([feature_dict[feat]*count for feat, count in feature_count.items()])\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-139.57175855522826"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_instance = \"This is the second U.N.-Congolese offensive against militias in the region since the DRC 's constitutional referendum a week ago .\".split()\n",
    "y_instance = 'O O O O O O O O O O O O O B-geo O O O O O O O'.split()\n",
    "compute_score(x_instance, y_instance, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x_sent, f_weights, tags):\n",
    "\n",
    "    # initialize\n",
    "    q1 = list()\n",
    "    for i in range(len(tags)):\n",
    "        score = 0\n",
    "        t_key = \"transition:START+\" + tags[i]\n",
    "#         print(\"test 1: \", t[t_key])\n",
    "        score += f_weights[t_key]\n",
    "        e_key = \"emission:\" + tags[i] + \"+\" + x_sent[0]\n",
    "#         print(\"test 2: \", e[e_key])\n",
    "        score += f_weights[e_key]\n",
    "        q1.append((i,score))\n",
    "\n",
    "    argmax = list()\n",
    "    q2 = []\n",
    "    for i in range(1, len(x_sent)):\n",
    "        argmax_sub = list()\n",
    "        for j in range(len(tags)):\n",
    "            scores = list()\n",
    "            for (k, last_score) in q1:\n",
    "                score = 0\n",
    "                t_key = \"transition:\" + tags[k] + \"+\" + tags[j]\n",
    "                score += f_weights[t_key]\n",
    "                e_key = \"emission:\" + tags[j] + \"+\" + x_sent[i]\n",
    "                score += f_weights[e_key]\n",
    "                scores.append(score+last_score)\n",
    "            best_score = np.max(scores)\n",
    "            best_tag = np.argmax(scores)\n",
    "            q2.append((best_tag, best_score))\n",
    "            argmax_sub.append(best_tag)\n",
    "        argmax.append(argmax_sub)\n",
    "        q1 = q2\n",
    "        q2 = []\n",
    "\n",
    "    scores = list()\n",
    "    for (k, last_score) in q1:\n",
    "        score = 0\n",
    "        t_key = \"transition:\" + tags[k] + \"+STOP\"\n",
    "        score += f_weights[t_key]\n",
    "        scores.append(score+last_score)\n",
    "    final_best_score = np.max(scores)\n",
    "    final_best_tag = np.argmax(scores)\n",
    "\n",
    "    path = [final_best_tag]\n",
    "    pointer = final_best_tag\n",
    "    for i in range(len(argmax)-1, -1, -1):\n",
    "        pointer = argmax[i][pointer]\n",
    "        path.append(pointer)\n",
    "\n",
    "    return path[::-1], final_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2202 tokens with 238 phrases; found: 255 phrases; correct: 155.\n",
      "accuracy:  95.10%; precision:  60.78%; recall:  65.13%; FB1:  62.88\n",
      "              art: precision: 100.00%; recall: 100.00%; FB1: 100.00  1\n",
      "              geo: precision:  65.06%; recall:  73.97%; FB1:  69.23  83\n",
      "              gpe: precision:  93.94%; recall:  96.88%; FB1:  95.38  33\n",
      "              nat: precision: 100.00%; recall: 100.00%; FB1: 100.00  2\n",
      "              org: precision:  41.46%; recall:  36.96%; FB1:  39.08  41\n",
      "              per: precision:  36.96%; recall:  44.74%; FB1:  40.48  46\n",
      "              tim: precision:  67.35%; recall:  71.74%; FB1:  69.47  49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evals = []\n",
    "# for i in range(len(X)):\n",
    "for i in range(len(X)-600):\n",
    "    path_idx, score = viterbi(X[i], W, Y_NER_set)\n",
    "    for j in range(len(path_idx)):\n",
    "        line = \"{} {} {} {}\".format(X[i][j], Y_POS[i][j], Y_NER[i][j], Y_NER_set[path_idx[j]])\n",
    "        evals.append(line)\n",
    "res = conlleval.evaluate(evals)\n",
    "print(conlleval.report(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (i): Calculate Loss by  forward algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# np.set_printoptions(precision=2)\n",
    "\n",
    "# def f1(y1, y2, x):\n",
    "#     return \"transition:\"+y1+\"+\"+y2\n",
    "# def f2(y1, y2, x):\n",
    "#     if y2 == \"STOP\":\n",
    "#         return 0\n",
    "# #     print(\"emission WTD:\"+y2+\"+\"+x)\n",
    "#     return \"emission:\"+y2+\"+\"+x\n",
    "\n",
    "# def M(y1, y2, x, W, f_ls):\n",
    "#     output = 0\n",
    "#     for f in f_ls:\n",
    "#         output += W[f(y1, y2, x)]\n",
    "#     return output\n",
    "# def forward_end(alpha, W, f_ls, Y_NER_set):\n",
    "#     m = len(Y_NER_set)\n",
    "        \n",
    "#     M_stop = np.zeros(m)\n",
    "#     for i in range(m):\n",
    "#         M_stop[i] = M(y1=Y_NER_set[i], y2='STOP', x=None, W=W, f_ls=f_ls[:1])\n",
    "#     mM_stop = M_stop.reshape(m)\n",
    "    \n",
    "# #     M_max = np.max(mM_stop)\n",
    "# #     mM_stop -= M_max    \n",
    "# #     mM_stop = np.exp(mM_stop)\n",
    "        \n",
    "# #     A_max = np.max(alpha)\n",
    "# #     alpha -= A_max\n",
    "# #     alpha = np.exp(alpha)\n",
    "    \n",
    "# #     alpha = np.log(np.dot(mM_stop, alpha)) + M_max + A_max\n",
    "\n",
    "#     alpha = alpha.reshape(m)\n",
    "#     alpha = mM_stop + alpha\n",
    "#     l_max = np.max(alpha)\n",
    "#     alpha -= l_max\n",
    "#     alpha = np.exp(alpha)\n",
    "#     alpha = np.sum(alpha)\n",
    "#     alpha = np.log(alpha)\n",
    "#     alpha = alpha + l_max\n",
    "    \n",
    "#     return alpha.item()  \n",
    "\n",
    "# def f11(mM_mid, alpha, m):\n",
    "#     a = alpha.reshape(1, -1)\n",
    "#     a = mM_mid + a\n",
    "#     l_max = np.max(a, axis=1).reshape(-1, 1)\n",
    "#     a -= l_max\n",
    "#     a = np.exp(a)\n",
    "#     a = np.log(np.sum(a,axis=1).reshape(-1, 1))\n",
    "#     a = a + l_max\n",
    "#     return a\n",
    "\n",
    "# def f12(mM_mid, alpha, m):\n",
    "#     a = alpha.reshape((m, 1))\n",
    "#     M_max = np.max(mM_mid, axis=1).reshape((m, 1))\n",
    "#     mM_mid -= M_max\n",
    "#     mM_mid = np.exp(mM_mid)\n",
    "        \n",
    "#     A_max = np.max(a)\n",
    "#     a -= A_max\n",
    "#     a = np.exp(a)\n",
    "#     a = np.matmul(mM_mid, a)\n",
    "#     a[a == 0] = 1\n",
    "#     a = np.log(a) + M_max + A_max\n",
    "#     return a\n",
    "\n",
    "# def forward(x_sent, W, f_ls, Y_NER_set, idx):\n",
    "#     # build M (start M->m*1; mid M->m*m; stop M->m*1)\n",
    "#     m = len(Y_NER_set)\n",
    "#     M_start = np.zeros(m)\n",
    "#     for i in range(m):\n",
    "#         M_start[i] = M(y1='START', y2=Y_NER_set[i], x=x_sent[0], W=W, f_ls=f_ls[:])\n",
    "    \n",
    "#     alpha = M_start.reshape((m,1))\n",
    "    \n",
    "#     M_mid = np.zeros((m, m))\n",
    "#     for i in range(m):\n",
    "#         for j in range(m):\n",
    "#             t_score = W.get( f\"transition:{Y_NER_set[j]}+{Y_NER_set[i]}\", ninf)\n",
    "# #             M_mid[i][j] = M(y1=Y_NER_set[j], y2=Y_NER_set[i], x=None, W=W, f_ls=f_ls[:1])\n",
    "#             M_mid[i][j] = t_score\n",
    "    \n",
    "#     for i in range(1, idx):\n",
    "#         mM_mid = copy.deepcopy(M_mid)\n",
    "#         for j in range(m):\n",
    "#             e_score = W.get( f\"emission:{Y_NER_set[j]}+{x_sent[i]}\", ninf)\n",
    "#             s = e_score\n",
    "# # mM_mid[j,:] += M(y1=None, y2=Y_NER_set[j], x=x_sent[i], W=W, f_ls=f_ls[1:])\n",
    "#             mM_mid[:,j] += s\n",
    "\n",
    "# #         M_max = np.max(mM_mid, axis=1).reshape((m, 1))\n",
    "# #         mM_mid -= M_max\n",
    "# #         mM_mid = np.exp(mM_mid)\n",
    "        \n",
    "# #         alpha = alpha.reshape((m,1))\n",
    "# #         A_max = np.max(alpha)\n",
    "# #         alpha -= A_max\n",
    "# #         alpha = np.exp(alpha)\n",
    "# #         alpha = np.log(np.matmul(mM_mid, alpha)+1) + M_max + A_max\n",
    "        \n",
    "# #         alpha = alpha.reshape(1, -1)\n",
    "# #         alpha = mM_mid + alpha\n",
    "# #         l_max = np.max(alpha, axis=1).reshape(-1, 1)\n",
    "# #         alpha -= l_max\n",
    "# #         alpha = np.exp(alpha)\n",
    "# #         alpha = np.log(np.sum(alpha,axis=1).reshape(-1, 1))\n",
    "# #         alpha = alpha + l_max\n",
    "#             alpha = f11(mM_mid, alpha, m=m)\n",
    "\n",
    "#     return alpha\n",
    "\n",
    "\n",
    "# def get_loss(X, Y_NER, W, f_ls):\n",
    "#     loss = 0\n",
    "#     for x_sent, y_sent in zip(X, Y_NER):\n",
    "#         score = compute_score(x_sent, y_sent, W)\n",
    "#         alpha = forward(x_sent, W, f_ls, Y_NER_set, len(x_sent))\n",
    "#         forward_score = forward_end(alpha, W, f_ls, Y_NER_set)\n",
    "#         loss += forward_score - score\n",
    "# #         print(\"score:{}, f:{}\".format(score, forward_score))\n",
    "# #         print(\"------------\")\n",
    "#     return loss\n",
    "\n",
    "# get_loss(X[1:2], Y_NER[1:2], W, [f1, f2])\n",
    "# # get_loss([x_instance], [y_instance], W, [f1, f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(a):\n",
    "    b = a.max()\n",
    "    return  b + np.log( (np.exp(a-b)).sum() )\n",
    "\n",
    "def feature_emission(y_prev, y_now, x_sent, W, idx):\n",
    "    if idx == 0:\n",
    "        return 'NOT IN'\n",
    "    else:\n",
    "        key = f\"emission:{y_prev}+{x_sent[idx-1]}\"\n",
    "        return key\n",
    "    \n",
    "def feature_transition(y_prev, y_now, x_sent, W, idx):\n",
    "    key = f\"transition:{y_prev}+{y_now}\"\n",
    "    return key\n",
    "\n",
    "f_ls = [feature_emission, feature_transition]\n",
    "\n",
    "def forward_(x_instance, y_vocab, W, feature_ls=f_ls):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.zeros( (n,d) )\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        score = 0\n",
    "        for f in feature_ls:\n",
    "            score += W.get(f(y_prev='START', y_now=y, x_sent=x_instance, W=W, idx=0), 0)\n",
    "        scores[0, i] = score\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            temp = copy.deepcopy(scores[i-1, :]) \n",
    "            for y_prev_i, y_prev in enumerate(y_vocab):\n",
    "                score = 0\n",
    "                for f in feature_ls:\n",
    "                    score += W.get(f(y_prev=y_prev, y_now=y, x_sent=x_instance, W=W, idx=i), 0)\n",
    "                temp[y_prev_i] += score\n",
    "            scores[i, y_i] = logsumexp(temp)\n",
    "    \n",
    "    temp = copy.deepcopy(scores[-1, :])\n",
    "    for i, y_prev in enumerate(y_vocab):\n",
    "        score = 0\n",
    "        for f in feature_ls:\n",
    "            score += W.get(f(y_prev=y_prev, y_now='STOP', x_sent=x_instance, W=W, idx=n), 0)\n",
    "        temp[i] += score\n",
    "    alpha = logsumexp(np.array(temp))\n",
    "    \n",
    "    return scores, alpha\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn_instance_(x_instance, y_instance, feature_dict, y_vocab):\n",
    "    first_term = compute_score(x_instance, y_instance, feature_dict)\n",
    "    _, forward_score = forward_(x_instance, y_vocab, feature_dict, feature_ls=f_ls)\n",
    "    return forward_score - first_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x_instance, y_vocab, feature_dict):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.zeros( (n,d) )\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:START+{y}\", ninf)\n",
    "        scores[0, i] = t_score\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            temp = []\n",
    "            for y_prev_i, y_prev in enumerate(y_vocab):\n",
    "                t_score = feature_dict.get( f\"transition:{y_prev}+{y}\", ninf)\n",
    "                e_score = feature_dict.get( f\"emission:{y_prev}+{x_instance[i-1]}\", ninf)\n",
    "                temp.append(e_score + t_score + scores[i-1, y_prev_i])\n",
    "            scores[i, y_i] = logsumexp(np.array(temp))\n",
    "    \n",
    "    temp = []\n",
    "    for i, y_prev in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:{y_prev}+STOP\", ninf)\n",
    "        e_score = feature_dict.get( f\"emission:{y_prev}+{x_instance[-1]}\", ninf)\n",
    "        temp.append(e_score + t_score + scores[-1, i])\n",
    "    alpha = logsumexp(np.array(temp))\n",
    "    \n",
    "    return scores, alpha\n",
    "\n",
    "def backward(x_instance, y_vocab, feature_dict, aggreg_fn=logsumexp):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.zeros( (n,d) )\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:{y}+STOP\", ninf)\n",
    "        e_score = feature_dict.get( f\"emission:{y}+{x_instance[-1]}\", ninf)\n",
    "        scores[-1, i] = t_score + e_score\n",
    "        \n",
    "    for i in range(n-1, 0, -1):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            temp = []\n",
    "            for y_next_i, y_next in enumerate(y_vocab):\n",
    "                t_score = feature_dict.get( f\"transition:{y}+{y_next}\", ninf)\n",
    "                e_score = feature_dict.get( f\"emission:{y}+{x_instance[i-1]}\")\n",
    "                temp.append(e_score + t_score + scores[i, y_next_i])\n",
    "            scores[i-1, y_i] = aggreg_fn(np.array(temp))\n",
    "            \n",
    "    temp = []\n",
    "    for i, y_next in enumerate(y_vocab):\n",
    "        t_score = feature_dict.get( f\"transition:START+{y_next}\")\n",
    "        temp.append(t_score + scores[0, i])\n",
    "    beta = aggreg_fn(np.array(temp))\n",
    "    \n",
    "    return scores, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (ii): Gradients by forward n backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def backward_(x_instance, y_vocab, W, feature_ls=f_ls):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    scores = np.zeros( (n,d) )\n",
    "    \n",
    "    for i, y in enumerate(y_vocab):\n",
    "        s = 0\n",
    "        for f in feature_ls:\n",
    "            s += W.get(f(y_prev=y, y_now='STOP', x_sent=x_instance, W=W, idx=n), 0)\n",
    "        scores[-1, i] = s\n",
    "        \n",
    "        \n",
    "    for i in range(n-1, 0, -1):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            temp = copy.deepcopy(scores[i,:])\n",
    "            for y_next_i, y_next in enumerate(y_vocab):\n",
    "                s = 0\n",
    "                for f in feature_ls:\n",
    "                    s += W.get(f(y_prev=y, y_now=y_next, x_sent=x_instance, W=W, idx=i), 0)\n",
    "                temp[y_next_i] += s\n",
    "            scores[i-1, y_i] = logsumexp(np.array(temp))\n",
    "            \n",
    "    temp = copy.deepcopy(scores[0,:])\n",
    "    for i, y_next in enumerate(y_vocab):\n",
    "        s = 0\n",
    "        for f in feature_ls:\n",
    "            s += W.get(f(y_prev='START', y_now=y_next, x_sent=x_instance, W=W, idx=0), 0)\n",
    "        temp[i] += s\n",
    "    beta = logsumexp(np.array(temp))\n",
    "    \n",
    "    return scores, beta\n",
    "\n",
    "\n",
    "def forward_backward(x_instance, y_vocab, feature_dict):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    f_scores, alpha = forward_(x_instance, y_vocab, feature_dict)\n",
    "    b_scores, beta = backward_(x_instance, y_vocab, feature_dict)\n",
    "    \n",
    "    feature_expected_count = defaultdict(float)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            e_feature = f\"emission:{y}+{x_instance[i]}\"\n",
    "            feature_expected_count[e_feature] += np.exp(f_scores[i, y_i] + b_scores[i, y_i] - alpha)\n",
    "            \n",
    "    for i, y_next in enumerate(y_vocab):\n",
    "        t_feature = f\"transition:START+{y_next}\"\n",
    "        feature_expected_count[t_feature] += np.exp(f_scores[0, i] + b_scores[0, i] - alpha)\n",
    "        \n",
    "        t_feature = f\"transition:{y_next}+STOP\"\n",
    "        feature_expected_count[t_feature] += np.exp(f_scores[-1, i] + b_scores[-1, i] - alpha)\n",
    "        \n",
    "    for y_i, y in enumerate(y_vocab):\n",
    "        for y_next_i, y_next in enumerate(y_vocab):\n",
    "            t_feature = f\"transition:{y}+{y_next}\"\n",
    "            t_score = feature_dict.get(t_feature, ninf)\n",
    "            total = 0\n",
    "            for i in range(n-1):\n",
    "                e_score = feature_dict.get(f\"emission:{y}+{x_instance[i]}\", ninf)\n",
    "                total += np.exp(f_scores[i, y_i] + b_scores[i+1, y_next_i] + t_score + e_score - alpha)\n",
    "            feature_expected_count[t_feature] = total\n",
    "            \n",
    "    return feature_expected_count\n",
    "\n",
    "def forward_backward_(x_instance, y_vocab, W, tf_ls=[feature_transition], sf_ls=[feature_emission]):\n",
    "    n, d = len(x_instance), len(y_vocab)\n",
    "    f_scores, alpha = forward_(x_instance, y_vocab, W, feature_ls=f_ls)\n",
    "    b_scores, beta = backward_(x_instance, y_vocab, W, feature_ls=f_ls)\n",
    "    \n",
    "    feature_expected_count = defaultdict(float)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for y_i, y in enumerate(y_vocab):\n",
    "            \n",
    "            for sf in sf_ls:\n",
    "                s_key = sf(y_prev=y, y_now=None, x_sent=x_instance, W=W, idx=i+1)\n",
    "                feature_expected_count[s_key] += np.exp(f_scores[i, y_i] + b_scores[i, y_i] - alpha)\n",
    "    \n",
    "    for i in range(n+1):\n",
    "        if i == 0:\n",
    "            for yi, y in enumerate(y_vocab):\n",
    "                for f in tf_ls:\n",
    "                    key = f(y_prev='START', y_now=y, x_sent=x_instance, W=W, idx=0)\n",
    "                    feature_expected_count[key] += np.exp(f_scores[i, yi] + b_scores[i, yi] - alpha)\n",
    "        elif i == n:\n",
    "            for yi, y in enumerate(y_vocab):\n",
    "                for f in tf_ls:\n",
    "                    key = f(y_prev=y, y_now='STOP', x_sent=x_instance, W=W, idx=n)\n",
    "                    feature_expected_count[key] += np.exp(f_scores[n-1, yi] + b_scores[n-1, yi] - alpha)\n",
    "        else:\n",
    "            for y_prev_i, y_prev in enumerate(y_vocab):\n",
    "                for y_now_i, y_now in enumerate(y_vocab):\n",
    "                    \n",
    "                    tkey_ls = []\n",
    "                    t_score = 0\n",
    "                    for tf in tf_ls:\n",
    "                        t_key = tf(y_prev=y_prev, y_now=y_now, x_sent=x_instance, W=W, idx=i)\n",
    "                        t_score += W.get(t_key, ninf)\n",
    "                        tkey_ls.append(t_key)\n",
    "                    s_score = 0\n",
    "                    for sf in sf_ls:\n",
    "                        s_key = sf(y_prev=y_prev, y_now=y_now, x_sent=x_instance, W=W, idx=i)\n",
    "                        s_score += W.get(s_key, ninf)\n",
    "                        \n",
    "                    for t_key in tkey_ls:\n",
    "                        feature_expected_count[t_key] += \\\n",
    "                            np.exp(f_scores[i-1, y_prev_i] + b_scores[i, y_now_i] + t_score + s_score - alpha)\n",
    "            \n",
    "    return feature_expected_count\n",
    "\n",
    "# TEST\n",
    "# y_vocab = Y_NER_set\n",
    "# print(forward(x_instance, y_vocab, W)[1])\n",
    "# print(forward_(x_instance, y_vocab, W)[1])\n",
    "# print(backward(x_instance, y_vocab, W)[1])\n",
    "# print(backward_(x_instance, y_vocab, W)[1])\n",
    "\n",
    "l1 = forward_backward(X[43], Y_NER_set, W)\n",
    "l2 = forward_backward_(X[43], Y_NER_set, W)\n",
    "# print(l1)\n",
    "# print(l2)\n",
    "print(l1==l2)\n",
    "l1 = list(set(forward_backward(X[23], Y_NER_set, W)))\n",
    "l2 = list(set(forward_backward_(X[23], Y_NER_set, W)))\n",
    "print(l1==l2)\n",
    "l1 = list(set(forward_backward(X[22], Y_NER_set, W)))\n",
    "l2 = list(set(forward_backward_(X[22], Y_NER_set, W)))\n",
    "print(l1==l2)\n",
    "\n",
    "def get_feature_count(x_instance, y_instance, feature_dict):\n",
    "    feature_count = defaultdict(int)\n",
    "    \n",
    "    for x, y in zip(x_instance, y_instance): \n",
    "        feature_count[f\"emission:{y}+{x}\"] += 1\n",
    "    \n",
    "    for y_prev, y in zip(['START'] + y_instance, y_instance + ['STOP']):\n",
    "        feature_count[f\"transition:{y_prev}+{y}\"] += 1\n",
    "    \n",
    "    return feature_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (i): Gradient and training with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_fn(x_data, y_data, feature_dict, y_vocab, eta=0.1):\n",
    "    feature_grad = defaultdict(float)\n",
    "    \n",
    "    for x_instance, y_instance in zip(x_data, y_data):\n",
    "        feature_expected_counts = forward_backward_(x_instance, y_vocab, feature_dict)\n",
    "        feature_actual_counts = get_feature_count(x_instance, y_instance, feature_dict)\n",
    "        for k, v in feature_expected_counts.items(): \n",
    "            feature_grad[k] += v\n",
    "        for k, v in feature_actual_counts.items(): \n",
    "            feature_grad[k] -= v    \n",
    "    \n",
    "    if eta > 0: \n",
    "        for k, v in feature_dict.items(): \n",
    "            feature_grad[k] += 2*eta*v\n",
    "    \n",
    "    return feature_grad\n",
    "        \n",
    "    \n",
    "def loss_fn(x_data, y_data, feature_dict, y_vocab, eta=0):\n",
    "    loss = 0\n",
    "    for x_instance, y_instance in zip(x_data, y_data):\n",
    "        loss += loss_fn_instance_(x_instance, y_instance, feature_dict, y_vocab) \n",
    "    reg_loss = eta * sum([o**2 for o in feature_dict.values()]) if eta > 0 else 0\n",
    "    return loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          emission:O+the passed gradient checking!\n",
      "                      transition:START+O passed gradient checking!\n",
      "                          transition:O+O passed gradient checking!\n",
      "                  transition:I-per+I-per passed gradient checking!\n"
     ]
    }
   ],
   "source": [
    "# Gradient verification\n",
    "feature_key_checks = ['emission:O+the', 'transition:START+O', 'transition:O+O', 'transition:I-per+I-per']\n",
    "feature_gradients = gradient_fn(X, Y_NER, W, Y_NER_set, eta=0)\n",
    "loss1 = loss_fn(X, Y_NER, W, Y_NER_set, eta=0)\n",
    "delta = 1e-6\n",
    "\n",
    "for feat_k in feature_key_checks:\n",
    "    new_W = W.copy()\n",
    "    new_W[feat_k] += delta\n",
    "    loss2 = loss_fn(X, Y_NER, new_W, Y_NER_set, eta=0)\n",
    "    numerical_grad = (loss2 - loss1) / delta\n",
    "    analytic_grad = feature_gradients[feat_k]\n",
    "    if abs(numerical_grad - analytic_grad) / max(abs(numerical_grad), 1e-8) < 1e-5: \n",
    "        print(f'{feat_k:>40} passed gradient checking!')\n",
    "    else:\n",
    "        print(f'{feat_k:>40} didnot pass gradient checking!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_dict(weight, feature_dict):\n",
    "    for i,k in enumerate(feature_dict.keys()):\n",
    "        feature_dict[k] = weight[i]\n",
    "    return feature_dict\n",
    "\n",
    "def dict_to_numpy(grads, feature_dict):\n",
    "    np_grads = np.zeros(len(feature_dict))\n",
    "    for i, k in enumerate(feature_dict.keys()):\n",
    "        np_grads[i] = grads[k]\n",
    "    return np_grads\n",
    "\n",
    "def get_loss_grad(weight, *args):\n",
    "    x_data, y_data, feature_dict, y_vocab = args\n",
    "    X, Y_NER, W, Y_NER_set = args\n",
    "    feature_dict = numpy_to_dict(weight, W)\n",
    "    loss = loss_fn(X, Y_NER, W, Y_NER_set, eta=0.1)\n",
    "    grads = gradient_fn(X, Y_NER, W, Y_NER_set, eta=0.1)\n",
    "    grads = dict_to_numpy(grads, W)\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "def callbackF(weight): print(f'Loss: \\t {loss_fn(X, Y_NER, W, Y_NER_set, eta=0.1):.4f}')\n",
    "\n",
    "# Initialization\n",
    "init_weight = np.zeros(len(W))\n",
    "feature_dict = numpy_to_dict(init_weight, W)\n",
    "\n",
    "\n",
    "# Training\n",
    "optimal_w, final_loss, _ = fmin_l_bfgs_b( \n",
    "    get_loss_grad, init_weight, pgtol=0.01, callback=callbackF,\n",
    "    args=(X, Y_NER, W, Y_NER_set) \n",
    ")\n",
    "\n",
    "# Save weights\n",
    "feature_dict = numpy_to_dict(optimal_w, W)\n",
    "weight_name = save_dir/'partial-part4-1.json'\n",
    "with open(weight_name, 'w') as f: \n",
    "    json.dump(W, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (ii): write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_name = save_dir/'partial-part4-1.json'\n",
    "# with open(weight_name) as f: feature_dict = json.load(f)\n",
    "\n",
    "# y_preds = inference(partial_dir/'dev.in', y_vocab, feature_dict, partial_dir/'dev.p4.out')\n",
    "# y_label = read_data(partial_dir/'dev.out', column=1)\n",
    "\n",
    "# y_label = [oo for o in y_label for oo in o+['O']]\n",
    "# y_preds = [oo for o in y_preds for oo in o+['O']]\n",
    "\n",
    "# prec, rec, f1 = evaluate(y_label, y_preds, verbose=False)\n",
    "# print(f'precision: {prec:.3f} \\t rec: {rec:.3f} \\t f1 {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (i) & (ii): POS & Combined feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_POS(y_prev, y_now, x_sent, W, idx):\n",
    "    return f\"emission:{y_prev}+{x_sent[idx-1]}\"\n",
    "\n",
    "def feature_combine(y_prev, y_now, x_sent, W, idx):\n",
    "    return f\"combine:{y_prev}+{y_now}+{x_sent[idx]}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 (i): Design new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
